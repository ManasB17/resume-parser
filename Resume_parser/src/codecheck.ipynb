{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "import re\n",
    "from tika import parser\n",
    "tika.initVM()\n",
    "\n",
    "\n",
    "def extract_text_from_resume(file_path):\n",
    "    results = parser.from_file(filename=file_path)\n",
    "    document_text = results['content']\n",
    "    \n",
    "    document_text = remove_metadata(document_text)\n",
    "    return document_text\n",
    "\n",
    "def remove_metadata(text):\n",
    "    # Define regular expressions to match metadata and unwanted generic strings\n",
    "    # regex_list = []\n",
    "    regex_list = [\n",
    "        r'^Title:.*$',\n",
    "        r'^Author:.*$',\n",
    "        r'^CreationDate:.*$',\n",
    "        r'^ModDate:.*$',\n",
    "        r'^Producer:.*$',\n",
    "        r'^Keywords:.*$',\n",
    "        r'^Subject:.*$',\n",
    "        r'^Content-Type:.*$',\n",
    "        r'^Resume.*$',\n",
    "        r'^CV.*$',\n",
    "    ]\n",
    "    \n",
    "    # Remove matching patterns from the text\n",
    "    for regex in regex_list:\n",
    "        text = re.sub(regex, '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhatt\\OneDrive\\Desktop\\desktop_files\\program\\internship\\NLP\\nlp_env\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bhatt\\OneDrive\\Desktop\\desktop_files\\program\\internship\\NLP\\nlp_env\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bhatt\\OneDrive\\Desktop\\desktop_files\\program\\internship\\NLP\\nlp_env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\bhatt\\OneDrive\\Desktop\\desktop_files\\program\\internship\\NLP\\nlp_env\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\bhatt\\OneDrive\\Desktop\\desktop_files\\program\\internship\\NLP\\nlp_env\\lib\\site-packages\\cupy\\_environment.py:213: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "def extract_name(nlp, text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Initialize an empty dictionary to store the extracted names\n",
    "    names = {\n",
    "        'first_name': '',\n",
    "        'middle_name': '',\n",
    "        'last_name': ''\n",
    "    }\n",
    "\n",
    "    # Look for entities in the text that are labeled as a person (PERSON)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Split the entity text into tokens and determine the first, middle, and last names\n",
    "            tokens = ent.text.split()\n",
    "            if len(tokens) == 1:\n",
    "                # If there is only one token, assume it is the first name\n",
    "                names['first_name'] = tokens[0]\n",
    "            elif len(tokens) == 2:\n",
    "                # If there are two tokens, assume the first is the first name and the second is the last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['last_name'] = tokens[1]\n",
    "            elif len(tokens) == 3:\n",
    "                # If there are three tokens, assume the first is the first name, the second is the middle name, and the third is the last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['middle_name'] = tokens[1]\n",
    "                names['last_name'] = tokens[2]\n",
    "            else:\n",
    "                # If there are more than three tokens, assume the last three are the middle and last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['middle_name'] = ' '.join(tokens[1:-1])\n",
    "                names['last_name'] = tokens[-1]\n",
    "\n",
    "            break\n",
    "\n",
    "    return names\n",
    "\n",
    "\n",
    "def extract_contact_info(text):\n",
    "    # Extract phone number using regular expression\n",
    "    phone_regex = r'\\b(?:\\d[ -.]*){9,}\\b'\n",
    "    phone_number = re.findall(phone_regex, text)\n",
    "\n",
    "    # Extract email address using regular expression\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    email = re.findall(email_regex, text)\n",
    "\n",
    "    contact_info = {\n",
    "        'phone': phone_number,\n",
    "        'email': email\n",
    "    }\n",
    "\n",
    "    return contact_info\n",
    "\n",
    "\n",
    "def extract_skills(nlp, resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "\n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"../data/skills.csv\")\n",
    "\n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "\n",
    "    skillset = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "\n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "    patterns = [nlp(text.lower()) for text in skills]\n",
    "    matcher.add('Skill', None, *patterns)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        skillset.append(span.text)\n",
    "\n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_resume(resume_text):\n",
    "    # Extract name\n",
    "    name = extract_name(nlp, resume_text)\n",
    "\n",
    "    # Extract contact info\n",
    "    contact_info = extract_contact_info(resume_text)\n",
    "\n",
    "    # Extract skills\n",
    "    skills = extract_skills(nlp, resume_text)\n",
    "\n",
    "    # # Extract education\n",
    "    # education = extract_education(resume_text)\n",
    "\n",
    "    # # Extract work experience\n",
    "    # work_experience = extract_work_experience(resume_text)\n",
    "\n",
    "    # Create a dictionary containing all the extracted data fields\n",
    "    resume_dict = {\n",
    "        'name': name,\n",
    "        'contact_info': contact_info,\n",
    "        'skills': skills,\n",
    "        # 'education': education,\n",
    "        # 'work_experience': work_experience\n",
    "    }\n",
    "\n",
    "    return resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_trf')\n",
    "file_path = '../data/Resume/mb.pdf'\n",
    "extracted_text = extract_text_from_resume(file_path)\n",
    "doc = nlp(extracted_text)\n",
    "text = \" \".join(\n",
    "    [token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "clean_text = re.sub('\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': {'first_name': 'Manas', 'middle_name': '', 'last_name': 'Bhattarai'}, 'contact_info': {'phone': ['9779862304880 ', '2011 12 2013 14'], 'email': ['bhattaraimanas@gmail.com', 'bhattaraimanas@gmail.com']}, 'skills': ['Pillow', 'Keras', 'R', 'Analysis', 'Windows', 'Unix', 'Flask', 'Tkinter', 'Java', 'English', 'Css', 'Matplotlib', 'Linux', 'Javascript', 'Tableau', 'Numpy', 'Python', 'Tensorflow', 'Excel', 'International', 'Cloud', 'Mysql', 'Opencv', 'Engineering', 'System', 'Networking', 'C++', 'Technical', 'Research', 'Github', 'Programming', 'Database', 'C', 'Email', 'Html']}\n"
     ]
    }
   ],
   "source": [
    "resume_info = extract_resume(clean_text)\n",
    "\n",
    "print(resume_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Anish Dahal \\n \\n\\n Address Sanepa Lalitpur \\n Contact Number +977 9817535981 \\n Email anishdahal441@gmail.com \\n LinkedIn https://www.linkedin.com/in/anish-dahal/ \\n\\n \\n\\n SUMMARY \\n \\n Passionate learn new challenging things Hardworking dedicated motivated achieve \\n perfection \\n\\n \\n\\n EDUCATION \\n \\n IOE Thapathali Campus Kathmandu \\n Bachelors Electronics Communication Engineering \\n Nov 2017 Present \\n\\n \\n CCRC Kathmandu \\n +2 \\n Jun 2015 2017 \\n\\n \\n Vishwa Jyoti Higher Secondary School \\n SLC \\n Apr 2003 2015 \\n\\n \\n\\n PROJECTS \\n \\n Minor Project Dynamic Maze Solving D*-Lite Dead End Exclusion Algorithm \\n group project 4 people incorporated bot maze design Webots application \\n C++ programming language implementing algorithms maze exploring \\n maze solving \\n\\n \\n Major Project Data Driven Approach Isolating Vocals Instruments Music \\n year long project methods Signal processing approach \\n Machine Learning approach Python Programming Language completing project \\n\\n \\n\\n Training Certifications \\n \\n WorldQuant University \\n Applied Data Science Lab \\n 2022 \\n https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile \\n\\n \\n Coursera \\n Deep Learning Specialization \\n 2022 \\n https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q \\n\\n mailto:anishdahal441@gmail.com subject=Email \\n https://www.linkedin.com/in/anish-dahal/ \\n https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile \\n https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q \\n\\n\\n \\n Databases Data Scientists Specialization \\n 2022 \\n https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH \\n\\n \\n freeCodeCamp \\n Data Analysis Python \\n 2022 \\n https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7 \\n\\n \\n\\n Skills \\n \\n Python SQL C++ DBMS Data Science Data Analysis Machine Learning \\n\\n \\n\\n https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH \\n https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7'\n"
     ]
    }
   ],
   "source": [
    "from extract_resume_info import get_clean_txt\n",
    "\n",
    "# define path to resume file\n",
    "resume_file_path = \"../data/Resume/dada.pdf\"\n",
    "\n",
    "# call get_clean_txt() function to extract and clean text from the resume file\n",
    "resume_text = get_clean_txt(resume_file_path)\n",
    "\n",
    "# print the cleaned text\n",
    "print(resume_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
