{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_resume(file_path):\n",
    "    results = parser.from_file(filename=file_path)\n",
    "    document_text = results['content']\n",
    "    return document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(extracted_text):\n",
    "    \n",
    "    # Define regex pattern to match list symbols and punctuation\n",
    "    pattern = r'^[\\s\\t]*[•*+\\-–>❖➢★■›»○✓✔✗✘x;:,�()\\'\\\"“”‘’][\\s\\t]+'\n",
    "\n",
    "    # Remove list symbols and punctuation using regex\n",
    "    clean_text = re.sub(pattern, '', extracted_text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove stop words and extra spaces\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(clean_text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    clean_text = ' '.join(filtered_words)\n",
    "\n",
    "    # Remove extra spaces between email addresses\n",
    "    email_pattern = r'(?<![^\\s])\\s*([A-Za-z0-9._%+-]+)\\s*@\\s*([A-Za-z0-9.-]+\\.[A-Z|a-z]{2,})(?![^\\s])'\n",
    "    clean_text = re.sub(email_pattern, r'\\1@\\2', clean_text)\n",
    "\n",
    "    # Remove periods that are not part of a sentence or email address\n",
    "    clean_text = re.sub(r'(?<!@)\\.(?!\\w|\\s)', '', clean_text)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_name(resume_text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    matcher = Matcher(nlp.vocab)   \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN', 'OP': '+'}]\n",
    "    matcher.add('FULL_NAME', [pattern])\n",
    "\n",
    "    doc = nlp(resume_text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        full_name = doc[start:end]\n",
    "        # Make sure the full name corresponds to a single person name\n",
    "        if len(full_name) > 1:\n",
    "            continue\n",
    "        print('Full name:', full_name.text)\n",
    "        print('First name:', full_name.text)\n",
    "\n",
    "# def extract_name(resume_text):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     matcher = Matcher(nlp.vocab)   \n",
    "#     # First name and Last name are always Proper Nouns\n",
    "#     pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN', 'OP': '?'}]\n",
    "#     matcher.add('FULL_NAME', [pattern])\n",
    "\n",
    "\n",
    "#     doc = nlp(resume_text)\n",
    "#     matches = matcher(doc)\n",
    "#     for match_id, start, end in matches:\n",
    "#         full_name = doc[start:end]\n",
    "#         print('Full name:', full_name.text)\n",
    "#         if len(full_name) == 2:\n",
    "#             first_name, last_name = full_name\n",
    "#             middle_name = None\n",
    "#         else:\n",
    "#             first_name, middle_name, last_name = full_name\n",
    "#         print('First name:', first_name.text)\n",
    "#         if middle_name is None:\n",
    "#             print('Middle name: None')\n",
    "#         else:\n",
    "#             print('Middle name:', middle_name.text)\n",
    "#         print('Last name:', last_name.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume ='../data/raw/dada.pdf'\n",
    "extracted_text = extract_text_from_resume(resume)\n",
    "clean_text = preprocess_text(extracted_text)\n",
    "extract_name(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
