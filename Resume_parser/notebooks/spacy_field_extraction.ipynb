{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "import spacy\n",
    "tika.initVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_resume(file_path):\n",
    "    results = parser.from_file(filename=file_path)\n",
    "    document_text = results['content']\n",
    "    return document_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting clean text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anish Dahal Address Sanepa Lalitpur Contact Number +977 9817535981 Email anishdahal441@gmail.com LinkedIn https://www.linkedin.com/in/anish-dahal/ SUMMARY Passionate learn new challenging things Hardworking dedicated motivated achieve perfection EDUCATION IOE Thapathali Campus Kathmandu Bachelors Electronics Communication Engineering Nov 2017 Present CCRC Kathmandu +2 Jun 2015 2017 Vishwa Jyoti Higher Secondary School SLC Apr 2003 2015 PROJECTS Minor Project Dynamic Maze Solving D*-Lite Dead End Exclusion Algorithm group project 4 people incorporated bot maze design Webots application C++ programming language implementing algorithms maze exploring maze solving Major Project Data Driven Approach Isolating Vocals Instruments Music year long project methods Signal processing approach Machine Learning approach Python Programming Language completing project Training Certifications WorldQuant University Applied Data Science Lab 2022 https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile Coursera Deep Learning Specialization 2022 https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q mailto:anishdahal441@gmail.com?subject=Email https://www.linkedin.com/in/anish-dahal/ https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q Databases Data Scientists Specialization 2022 https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH freeCodeCamp Data Analysis Python 2022 https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7 Skills Python SQL C++ DBMS Data Science Data Analysis Machine Learning https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "resume = '../data/raw/dada.pdf'\n",
    "resume_text = extract_text_from_resume(resume)\n",
    "\n",
    "\n",
    "if resume in resume_text:\n",
    "    resume_text = resume_text.replace(filename, '')\n",
    "\n",
    "doc = nlp(resume_text)\n",
    "\n",
    "#remove file name\n",
    "# remove html tags\n",
    "text_without_html = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "text_without_html\n",
    "\n",
    "# remove unwanted white spaces\n",
    "\n",
    "clean_text = re.sub('\\s+', ' ', text_without_html).strip()\n",
    "clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remove metadata and give a clean code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anish Dahal Address Sanepa Lalitpur Contact Number +977 9817535981 Email anishdahal441@gmail.com LinkedIn https://www.linkedin.com/in/anish-dahal/ SUMMARY Passionate learn new challenging things Hardworking dedicated motivated achieve perfection EDUCATION IOE Thapathali Campus Kathmandu Bachelors Electronics Communication Engineering Nov 2017 Present CCRC Kathmandu +2 Jun 2015 2017 Vishwa Jyoti Higher Secondary School SLC Apr 2003 2015 PROJECTS Minor Project Dynamic Maze Solving D*-Lite Dead End Exclusion Algorithm group project 4 people incorporated bot maze design Webots application C++ programming language implementing algorithms maze exploring maze solving Major Project Data Driven Approach Isolating Vocals Instruments Music year long project methods Signal processing approach Machine Learning approach Python Programming Language completing project Training Certifications WorldQuant University Applied Data Science Lab 2022 https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile Coursera Deep Learning Specialization 2022 https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q mailto:anishdahal441@gmail.com?subject=Email https://www.linkedin.com/in/anish-dahal/ https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q Databases Data Scientists Specialization 2022 https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH freeCodeCamp Data Analysis Python 2022 https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7 Skills Python SQL C++ DBMS Data Science Data Analysis Machine Learning https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tika\n",
    "from tika import parser\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_text_from_resume(file_path):\n",
    "    # Extract text from PDF file\n",
    "    results = parser.from_file(filename=file_path)\n",
    "    document_text = results['content']\n",
    "    \n",
    "    # Remove metadata from extracted text\n",
    "    document_text = remove_metadata(document_text)\n",
    "    \n",
    "    return document_text\n",
    "\n",
    "def remove_metadata(text):\n",
    "    # Define regular expressions to match metadata and unwanted generic strings\n",
    "    regex_list = []\n",
    "    # regex_list = [\n",
    "    #     r'^Title:.*$',\n",
    "    #     r'^Author:.*$',\n",
    "    #     r'^CreationDate:.*$',\n",
    "    #     r'^ModDate:.*$',\n",
    "    #     r'^Producer:.*$',\n",
    "    #     r'^Keywords:.*$',\n",
    "    #     r'^Subject:.*$',\n",
    "    #     r'^Content-Type:.*$',\n",
    "    #     r'^Resume.*$',\n",
    "    #     r'^CV.*$',\n",
    "    #     r'.*MYCV.*',\n",
    "    #     r'.*myresume.*'\n",
    "    #     r'.*Curriculum_Vitae.*'\n",
    "    # ]\n",
    "    \n",
    "    # Remove matching patterns from the text\n",
    "    for regex in regex_list:\n",
    "        text = re.sub(regex, '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "resume = '../data/raw/dada.pdf'\n",
    "resume_text = extract_text_from_resume(resume)\n",
    "\n",
    "doc = nlp(resume_text)\n",
    "\n",
    "#remove file name\n",
    "# remove html tags\n",
    "text_without_html = \" \".join([token.text for token in doc if not token.is_stop and not token.is_punct])\n",
    "text_without_html\n",
    "\n",
    "# remove unwanted white spaces\n",
    "\n",
    "clean_text = re.sub('\\s+', ' ', text_without_html).strip()\n",
    "clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting fields from the documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dahal Address', 'Address Sanepa', 'Sanepa Lalitpur', 'Lalitpur Contact', 'Email anishdahal441@gmail.com', 'anishdahal441@gmail.com LinkedIn', 'LinkedIn https://www.linkedin.com/in/anish-dahal/', 'https://www.linkedin.com/in/anish-dahal/ SUMMARY', 'SUMMARY Passionate', 'EDUCATION IOE', 'IOE Thapathali', 'Thapathali Campus', 'Campus Kathmandu', 'Kathmandu Bachelors', 'Bachelors Electronics', 'Electronics Communication', 'Communication Engineering', 'Engineering Nov', 'Kathmandu +2', '+2 Jun', 'Vishwa Jyoti', 'Jyoti Higher', 'Higher Secondary', 'Secondary School', 'School SLC', 'SLC Apr', 'Minor Project', 'Project Dynamic', 'Dynamic Maze', 'Maze Solving', 'Solving D*-Lite', 'D*-Lite Dead', 'Exclusion Algorithm', 'Major Project', 'Project Data', 'Data Driven', 'Driven Approach', 'Approach Isolating', 'Isolating Vocals', 'Vocals Instruments', 'Instruments Music', 'Machine Learning', 'Python Programming', 'Programming Language', 'Training Certifications', 'Certifications WorldQuant', 'WorldQuant University', 'University Applied', 'Applied Data', 'Data Science', 'Science Lab', 'Coursera Deep', 'Deep Learning', 'Learning Specialization', 'https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q', 'https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q Databases', 'Databases Data', 'Data Scientists', 'Scientists Specialization', 'Data Analysis', 'Analysis Python', 'Skills Python', 'Python SQL', 'SQL C++', 'C++ DBMS', 'DBMS Data', 'Data Science', 'Science Data', 'Data Analysis', 'Analysis Machine', 'Machine Learning', 'Learning https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH']\n"
     ]
    }
   ],
   "source": [
    "# extracting names form extracted text\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# loading pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initializing matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# def on_match(matcher, doc, id, matches):\n",
    "#     print('Matched!', matches)\n",
    "    \n",
    "def extract_name(txt):\n",
    "    pattern = [[{'POS': 'PROPN'}, {'POS': 'PROPN'}]]\n",
    "\n",
    "    matcher.add(\"FullName\", pattern)\n",
    "\n",
    "    doc = nlp(clean_text)\n",
    "    matches = matcher(doc)\n",
    "    names = []\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end].text\n",
    "        names.append(span)\n",
    "    return names\n",
    "name = extract_name(clean_text)\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lalitpur Contact\n"
     ]
    }
   ],
   "source": [
    "# extract name\n",
    "import spacy\n",
    "\n",
    "def extract_name(text):\n",
    "    # Load the English model with the transformer architecture\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize an empty dictionary to store the extracted names\n",
    "    names = {\n",
    "        'first_name': '',\n",
    "        'middle_name': '',\n",
    "        'last_name': ''\n",
    "    }\n",
    "    \n",
    "    # Look for entities in the text that are labeled as a person (PERSON)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            # Split the entity text into tokens and determine the first, middle, and last names\n",
    "            tokens = ent.text.split()\n",
    "            if len(tokens) == 1:\n",
    "                # If there is only one token, assume it is the first name\n",
    "                names['first_name'] = tokens[0]\n",
    "            elif len(tokens) == 2:\n",
    "                # If there are two tokens, assume the first is the first name and the second is the last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['last_name'] = tokens[1]\n",
    "            elif len(tokens) == 3:\n",
    "                # If there are three tokens, assume the first is the first name, the second is the middle name, and the third is the last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['middle_name'] = tokens[1]\n",
    "                names['last_name'] = tokens[2]\n",
    "            else:\n",
    "                # If there are more than three tokens, assume the last three are the middle and last name\n",
    "                names['first_name'] = tokens[0]\n",
    "                names['middle_name'] = ' '.join(tokens[1:-1])\n",
    "                names['last_name'] = tokens[-1]\n",
    "                \n",
    "            break\n",
    "    \n",
    "    return names\n",
    "\n",
    "names = extract_name(clean_text)\n",
    "if names['middle_name'] == '':\n",
    "     print(names['first_name'], names['last_name'])\n",
    "else:\n",
    "    print(names['first_name'], names['middle_name'],names['last_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phone': ['977 9817535981 '], 'email': ['anishdahal441@gmail.com', 'anishdahal441@gmail.com']}\n"
     ]
    }
   ],
   "source": [
    "# extract contact details\n",
    "def extract_contact_info(text):\n",
    "    # Extract phone number using regular expression\n",
    "    phone_regex = r'\\b(?:\\d[ -.]*){9,}\\b'\n",
    "    phone_number = re.findall(phone_regex, text)\n",
    "    \n",
    "    # Extract email address using regular expression\n",
    "    email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    email = re.findall(email_regex, text)\n",
    "        \n",
    "    contact_info = {\n",
    "        'phone': phone_number,\n",
    "        'email': email\n",
    "    }\n",
    "    \n",
    "    return contact_info\n",
    "contact = extract_contact_info(clean_text)\n",
    "print(contact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anish\n",
      "Dahal\n",
      "Address\n",
      "Sanepa\n",
      "Lalitpur\n",
      "Contact\n",
      "Number\n",
      "+977\n",
      "9817535981\n",
      "Email\n",
      "anishdahal441@gmail.com\n",
      "LinkedIn\n",
      "https://www.linkedin.com/in/anish-dahal/\n",
      "SUMMARY\n",
      "Passionate\n",
      "learn\n",
      "new\n",
      "challenging\n",
      "things\n",
      "Hardworking\n",
      "dedicated\n",
      "motivated\n",
      "achieve\n",
      "perfection\n",
      "EDUCATION\n",
      "IOE\n",
      "Thapathali\n",
      "Campus\n",
      "Kathmandu\n",
      "Bachelors\n",
      "Electronics\n",
      "Communication\n",
      "Engineering\n",
      "Nov\n",
      "2017\n",
      "Present\n",
      "CCRC\n",
      "Kathmandu\n",
      "+2\n",
      "Jun\n",
      "2015\n",
      "2017\n",
      "Vishwa\n",
      "Jyoti\n",
      "Higher\n",
      "Secondary\n",
      "School\n",
      "SLC\n",
      "Apr\n",
      "2003\n",
      "2015\n",
      "PROJECTS\n",
      "Minor\n",
      "Project\n",
      "Dynamic\n",
      "Maze\n",
      "Solving\n",
      "D*-Lite\n",
      "Dead\n",
      "End\n",
      "Exclusion\n",
      "Algorithm\n",
      "group\n",
      "project\n",
      "4\n",
      "people\n",
      "incorporated\n",
      "bot\n",
      "maze\n",
      "design\n",
      "Webots\n",
      "application\n",
      "C++\n",
      "programming\n",
      "language\n",
      "implementing\n",
      "algorithms\n",
      "maze\n",
      "exploring\n",
      "maze\n",
      "solving\n",
      "Major\n",
      "Project\n",
      "Data\n",
      "Driven\n",
      "Approach\n",
      "Isolating\n",
      "Vocals\n",
      "Instruments\n",
      "Music\n",
      "year\n",
      "long\n",
      "project\n",
      "methods\n",
      "Signal\n",
      "processing\n",
      "approach\n",
      "Machine\n",
      "Learning\n",
      "approach\n",
      "Python\n",
      "Programming\n",
      "Language\n",
      "completing\n",
      "project\n",
      "Training\n",
      "Certifications\n",
      "WorldQuant\n",
      "University\n",
      "Applied\n",
      "Data\n",
      "Science\n",
      "Lab\n",
      "2022\n",
      "https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile\n",
      "Coursera\n",
      "Deep\n",
      "Learning\n",
      "Specialization\n",
      "2022\n",
      "https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q\n",
      "mailto:anishdahal441@gmail.com?subject=Email\n",
      "https://www.linkedin.com/in/anish-dahal/\n",
      "https://www.credly.com/badges/d71ca3c0-4c9f-4937-99c9-845ab3522479/linked_in_profile\n",
      "https://www.coursera.org/account/accomplishments/specialization/certificate/9BXJCBUTST3Q\n",
      "Databases\n",
      "Data\n",
      "Scientists\n",
      "Specialization\n",
      "2022\n",
      "https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH\n",
      "freeCodeCamp\n",
      "Data\n",
      "Analysis\n",
      "Python\n",
      "2022\n",
      "https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7\n",
      "Skills\n",
      "Python\n",
      "SQL\n",
      "C++\n",
      "DBMS\n",
      "Data\n",
      "Science\n",
      "Data\n",
      "Analysis\n",
      "Machine\n",
      "Learning\n",
      "https://www.coursera.org/account/accomplishments/specialization/certificate/XMRMJPR4RVYH\n",
      "https://freecodecamp.org/certification/Anixh/data-analysis-with-python-v7\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "def tokenize(txt):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(clean_text)\n",
    "\n",
    "    for token in doc:\n",
    "        print(token.text)\n",
    "tokenize(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
